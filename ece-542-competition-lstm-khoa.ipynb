{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoQqrLoi11-9"
   },
   "source": [
    "# ECE 542 competition\n",
    "\n",
    "Click to the link to get more detail: https://research.ece.ncsu.edu/aros/paper-tase2020-lowerlimb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfT67p4_2D9V"
   },
   "source": [
    "## I. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bqxIAR4v-cL",
    "outputId": "21963b8e-547f-4ccd-aece-5934f045fd25"
   },
   "outputs": [],
   "source": [
    "\"\"\" **Description**\n",
    "        ECE 542 Competition is the competition project for the NCSU ECE 542 Neural\n",
    "        Networks course in the 2021 Fall semester.\n",
    "\n",
    "    **License**\n",
    "        Â© 2021 - 2021 Khoa Do, Duy Nguyen, Larry Turner. All rights reserved.\n",
    "\n",
    "    **Author**\n",
    "        Khoa Do.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Checking if CUDA is available\n",
    "flag_cuda = torch.cuda.is_available()\n",
    "\n",
    "if not flag_cuda:\n",
    "    print('Using CPU')\n",
    "else:\n",
    "    print('Using GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ik1G6GE11_E"
   },
   "source": [
    "### I.2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkxbhJoY2uhB"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "import glob\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zINgMdFn11_F"
   },
   "outputs": [],
   "source": [
    "class Data():\n",
    "\n",
    "    \"\"\" Data.  Load and aggregate data into a collection of data frames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.TEST = '../data/test'\n",
    "        self.TRAINING = '../data/training'\n",
    "\n",
    "        self.frequency = 0\n",
    "        self.test = [] \n",
    "        self.training = [] \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Initialize.\n",
    "        \"\"\"\n",
    "\n",
    "        for index__, path in enumerate(sorted(set(x.split('__')[0] for x in glob.glob(f'{self.TRAINING}/*__*.csv')))):\n",
    "            print(f\"Processs file: {path}\")\n",
    "            # get sample and time\n",
    "            x = pandas.read_csv(f'{path}__x.csv', names=(\n",
    "                'ax', 'ay', 'az', 'gx', 'gy', 'gz'))\n",
    "            t = pandas.read_csv(f'{path}__x_time.csv', names=('time', ))\n",
    "            x.insert(0, 'time', t)\n",
    "\n",
    "            # get subject_id and series_id from path and insert to self\n",
    "            [subject_id__, series_id__] = [\n",
    "                int(s) for s in re.findall(r'-?\\d+\\.?\\d*', path)]\n",
    "            subject_id = [subject_id__] * len(t)\n",
    "            series_id = [series_id__] * len(t)\n",
    "            index = [index__] * len(t)\n",
    "            x.insert(0, 'subject_id', subject_id)\n",
    "            x.insert(0, 'series_id', series_id)\n",
    "            x.insert(0, 'file_id', index)\n",
    "            # get frequency of signal\n",
    "            if (numpy.isclose(self.frequency, 0.0)):\n",
    "                self.frequency = numpy.round(\n",
    "                    1.0 / numpy.mean(x.time[1:].values - x.time[0: -1].values))\n",
    "            \n",
    "            y = pandas.read_csv(f'{path}__y.csv', names=('label', ))\n",
    "            t = pandas.read_csv(f'{path}__y_time.csv', names=('time', ))\n",
    "            y.insert(0, 'time', t)\n",
    "            x.insert(x.shape[1], 'label', y.label[len(y) - 1])\n",
    "            ii = 0\n",
    "            with pandas.option_context('mode.chained_assignment', None):\n",
    "                for jj in range(0, len(x)):\n",
    "                    x.label[jj] = y.label[ii]\n",
    "                    while ((ii < (len(y) - 1)) and (x.time[jj] >= y.time[ii + 1])):\n",
    "                        ii += 1\n",
    "            self.training.append(x)\n",
    "\n",
    "        # Test.\n",
    "        for index__, path in enumerate(sorted(set(x.split('__')[0] for x in glob.glob(f'{self.TEST}/*__*.csv')))):\n",
    "            print(f\"Processs file: {path}\")\n",
    "            x = pandas.read_csv(f'{path}__x.csv', names=(\n",
    "                'ax', 'ay', 'az', 'gx', 'gy', 'gz'))\n",
    "            t = pandas.read_csv(f'{path}__x_time.csv', names=('time', ))\n",
    "            x.insert(0, 'time', t)\n",
    "            x.insert(x.shape[1], 'label', -1)\n",
    "\n",
    "            # get subject_id and series_id from path and insert to self\n",
    "            [subject_id__, series_id__] = [\n",
    "                int(s) for s in re.findall(r'-?\\d+\\.?\\d*', path)]\n",
    "            subject_id = [subject_id__] * len(t)\n",
    "            series_id = [series_id__] * len(t)\n",
    "            index = [index__] * len(t)\n",
    "\n",
    "            x.insert(0, 'subject_id', subject_id)\n",
    "            x.insert(0, 'series_id', series_id)\n",
    "            x.insert(0, 'file_id', index)\n",
    "            self.test.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6h3aT-zL11_I",
    "outputId": "52a88f5e-2871-4271-fe57-23b91005de05",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataObj = Data()\n",
    "dataObj.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ANRXFu_N11_I",
    "outputId": "224ccff4-9a70-4688-a89c-9c5d9c431b7c"
   },
   "outputs": [],
   "source": [
    "# view the training data\n",
    "dataObj.training[0].head()\n",
    "#print(dataObj.training[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "dew6nwVS11_J",
    "outputId": "efb32095-c8fb-4d2c-c4f0-0797c86a3ac5"
   },
   "outputs": [],
   "source": [
    "# view the test data\n",
    "dataObj.test[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnIgpbm2-XDV"
   },
   "source": [
    "### [*] Run the bellow code to save/load data as csv file\n",
    "\n",
    "Since Google Colab is sooooo unstable, better save the preprocessed data into files and load them later for following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpfqqEAU11_K"
   },
   "outputs": [],
   "source": [
    "# SAVE THE DATA IN UNIQUE CSV FILE\n",
    "path_data_training = \"../data/output/data-training.csv\"\n",
    "path_data_test = \"../data/output/data-test.csv\"\n",
    "\n",
    "for i, df in enumerate(dataObj.training):\n",
    "    if i:\n",
    "        df.to_csv(path_data_training, mode='a', header=False, index=False)\n",
    "    else: \n",
    "        df.to_csv(path_data_training, mode='w', header=True, index=False)\n",
    "\n",
    "for i, df in enumerate(dataObj.test):\n",
    "    if i:\n",
    "        df.to_csv(path_data_test, mode='a', header=False, index=False)\n",
    "    else: \n",
    "        df.to_csv(path_data_test, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snXHYSNJ11_L"
   },
   "outputs": [],
   "source": [
    "# run the code to LOAD THE DATA from csv file\n",
    "path_data_training = \"../data/output/data-training.csv\"\n",
    "path_data_test = \"../data/output/data-test.csv\"\n",
    "\n",
    "dataObjBackUp = Data()\n",
    "data_training = pandas.read_csv(path_data_training)  \n",
    "for file_id in set(data_training[\"file_id\"].values):\n",
    "    data_training_part = data_training[data_training.file_id == file_id].sort_values(by=['time'])\n",
    "    dataObjBackUp.training.append(data_training_part)\n",
    "    \n",
    "data_test = pandas.read_csv(path_data_test)  \n",
    "for file_id in set(data_test[\"file_id\"].values):\n",
    "    data_test_part = data_test[data_test.file_id == file_id].sort_values(by=['time'])\n",
    "    dataObjBackUp.test.append(data_test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "hems4waM5278",
    "outputId": "0e552ba8-c7f1-4fcc-aeca-f433d5b022c0"
   },
   "outputs": [],
   "source": [
    "dataObjBackUp.training[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "wS7fO4Xg11_M",
    "outputId": "a27a4f44-8acd-45f7-f0c1-3a8629732e4c"
   },
   "outputs": [],
   "source": [
    "dataObjBackUp.test[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMT0ohzo11_M"
   },
   "source": [
    "### I.3. Apply Lowpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwWD0GplAkgg"
   },
   "outputs": [],
   "source": [
    "from diamondback import IirFilter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing import Dict, List, Tuple, Union\n",
    "import glob\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow4GoFWY11_M"
   },
   "outputs": [],
   "source": [
    "class Filter() :\n",
    "\n",
    "    \"\"\" Filter.  Filter and transform data frames.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "      self.FREQUENCY = 0.7\n",
    "      self.frequency : float = 40\n",
    "      self.test : List[pandas.DataFrame] = []\n",
    "      self.training : List[pandas.DataFrame] = []\n",
    "\n",
    "    \n",
    "    def run(self, Data) -> None :\n",
    "\n",
    "        \"\"\" Initialize.\n",
    "        \"\"\"\n",
    "        # run in the google colab\n",
    "        iir = IirFilter(style = 'Butterworth', frequency = self.FREQUENCY, order = 2)\n",
    "        \n",
    "        # run in local\n",
    "        # iir = IirFilter.Factory.instance(IirFilter, 'Butterworth', frequency = self.FREQUENCY, order = 2)\n",
    "        \n",
    "        delay = int(numpy.round(numpy.mean(iir.delay(16)[0])))\n",
    "        title = [u for u in Data.training[0].columns if (u not in ('file_id', 'subject_id', 'series_id', 'time', 'label'))]\n",
    "        gain = dict(zip(title, [0.2] * 3 + [1.0] * 3))\n",
    "\n",
    "        # Training\n",
    "        self.training = []\n",
    "        for i, x in enumerate(Data.training):\n",
    "            print(f\"Process Training file_id {i}\")\n",
    "            with pandas.option_context('mode.chained_assignment', None):\n",
    "                y = x.copy( )\n",
    "                for ii in [u for u in title]:\n",
    "                    v = iir.filter((y[ii] - numpy.mean(y[ii])) * gain[ii])\n",
    "                    y[ii] = numpy.concatenate((v[delay:], [v[-1]] * delay))\n",
    "                self.training.append(y)\n",
    "\n",
    "        # Test.\n",
    "        self.test = []\n",
    "        for i, x in enumerate(Data.test):\n",
    "            print(f\"Process Testing file_id {i}\")\n",
    "            with pandas.option_context( 'mode.chained_assignment', None ) :\n",
    "                y = x.copy( )\n",
    "                for ii in [u for u in title]:\n",
    "                    v = iir.filter((y[ii] - numpy.mean(y[ii])) * gain[ii])\n",
    "                    y[ii] = numpy.concatenate((v[delay:], [v[-1]] * delay))\n",
    "                self.test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thTIXoeP_ID8",
    "outputId": "a6943930-ce60-4094-e7bb-45a6232d2193"
   },
   "outputs": [],
   "source": [
    "filterObj = Filter()\n",
    "filterObj.run(dataObj)\n",
    "#filterObj.run(dataObjBackUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "H3uZVR0m11_N",
    "outputId": "a9d99f9b-4cfc-4cce-df7f-c6c5685efc89",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filterObj.training[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "VTCnKZGd11_N",
    "outputId": "1d5c2fa8-8281-4568-9694-84dd25845116"
   },
   "outputs": [],
   "source": [
    "filterObj.test[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEiZj0wbItLW"
   },
   "source": [
    "### [*] Run the bellow code to save/load filtered data to/from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef4mhvrq11_O"
   },
   "outputs": [],
   "source": [
    "# save the data as csv file\n",
    "path_filter_training = \"../data/output/filter-training.csv\"\n",
    "path_filter_test = \"../data/output/filter-test.csv\"\n",
    "\n",
    "for i, df in enumerate(filterObj.training):\n",
    "    if i:\n",
    "        df.to_csv(path_filter_training, mode='a', header=False, index=False)\n",
    "    else: \n",
    "        df.to_csv(path_filter_training, mode='w', header=True, index=False)\n",
    "\n",
    "for i, df in enumerate(filterObj.test):\n",
    "    if i:\n",
    "        df.to_csv(path_filter_test, mode='a', header=False, index=False)\n",
    "    else: \n",
    "        df.to_csv(path_filter_test, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osmHaNXg11_P"
   },
   "outputs": [],
   "source": [
    "# run the code to LOAD THE DATA from csv file\n",
    "path_filter_training = \"../data/output/filter-training.csv\"\n",
    "path_filter_test = \"../data/output/filter-test.csv\"\n",
    "\n",
    "filterObjBackUp = Filter()\n",
    "filter_training = pandas.read_csv(path_filter_training)  \n",
    "for file_id in set(filter_training[\"file_id\"].values):\n",
    "    filter_training_part = filter_training[filter_training.file_id == file_id].sort_values(by=['time'])\n",
    "    filterObjBackUp.training.append(filter_training_part)\n",
    "    \n",
    "filter_test = pandas.read_csv(path_filter_test)  \n",
    "for file_id in set(filter_training[\"file_id\"].values):\n",
    "    filter_test_part = filter_test[filter_test.file_id == file_id].sort_values(by=['time'])\n",
    "    filterObjBackUp.test.append(filter_test_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjniR5yk11_P"
   },
   "source": [
    "## II. Extract features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2BFe0gl11_P"
   },
   "outputs": [],
   "source": [
    "class Features() :\n",
    "\n",
    "    \"\"\" Features.  Extract features as windowed properties, moments, and norms, into a data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        self.FEATURES = 1\n",
    "        self.WINDOW = 4\n",
    "\n",
    "        self.test : List[pandas.DataFrame] = []\n",
    "        self.training : List[pandas.DataFrame] = []\n",
    "\n",
    "    \n",
    "    def run(self, Filter):\n",
    "\n",
    "        \"\"\" Initialize.\n",
    "        \"\"\"\n",
    "\n",
    "        title = [ u for u in Filter.training[0].columns if (u not in ('file_id', 'subject_id', 'series_id', 'time', 'label'))]\n",
    "\n",
    "        # Training.\n",
    "        # self.training = [ ]\n",
    "        for x in Filter.training :\n",
    "            file_id_list = set(x[\"file_id\"].values[:])\n",
    "            assert len(file_id_list) == 1, \"[Error] The File ID is not unique\"\n",
    "\n",
    "            file_id__ = x[\"file_id\"].values[0]\n",
    "            subject_id__ = x[\"subject_id\"].values[0]\n",
    "            series_id__ = x[\"series_id\"].values[0]               \n",
    "            print(\"------------\")\n",
    "            print(f\"file_id: {file_id__}, subject_id: {subject_id__}, series_id: {series_id__}\")           \n",
    "            with pandas.option_context( 'mode.chained_assignment', None ) :\n",
    "                y = pandas.DataFrame( columns = [ 'file_id', 'subject_id', 'series_id', 'time' ] + [ f'{u}{ii}' for u in title for ii in range( 0, self.FEATURES ) ] + [ 'label' ])\n",
    "                actual_len = round(len(x) / self.WINDOW) * self.WINDOW\n",
    "                for ii in range(0, actual_len, self.WINDOW ) :\n",
    "                    try:\n",
    "                        z = x[ ii : ii + self.WINDOW ]\n",
    "                    except:\n",
    "                        z = x[ ii : ]\n",
    "                    assert len(set(z[\"label\"].values[:])), \"[Error] Re-sampling is not OK\"\n",
    "\n",
    "                    feature = [file_id__, subject_id__, series_id__, z[\"time\"].values[0]]\n",
    "                    for v in [ np.array( z[u] ) for u in title ] :\n",
    "                        u = np.mean( v )\n",
    "                        feature += [u]\n",
    "                    feature += [z[\"label\"].values[0]]\n",
    "                    y.loc[len(y)] = feature\n",
    "                y = y.astype({'file_id': 'int64', \n",
    "                                'subject_id': 'int64',\n",
    "                                'series_id': 'int64',\n",
    "                                'label' : 'int64'})                 \n",
    "                self.training.append(y)\n",
    "\n",
    "        # Test.\n",
    "        self.test = []\n",
    "        for x in Filter.test :\n",
    "            file_id_list = set(x[\"file_id\"].values[:])\n",
    "            assert len(file_id_list) == 1, \"[Error] The File ID is not unique\"\n",
    "            \n",
    "            file_id__ = x[\"file_id\"].values[0]\n",
    "            subject_id__ = x[\"subject_id\"].values[0]\n",
    "            series_id__ = x[\"series_id\"].values[0]             \n",
    "            print(\"------------\")\n",
    "            print(f\"file_id: {file_id__}, subject_id: {subject_id__}, series_id: {series_id__}\")           \n",
    "            with pandas.option_context( 'mode.chained_assignment', None ) :\n",
    "                y = pandas.DataFrame( columns = [ 'file_id', 'subject_id', 'series_id', 'time' ] + [ f'{u}{ii}' for u in title for ii in range( 0, self.FEATURES ) ] + [ 'label' ])\n",
    "                \n",
    "                actual_len = round(len(x) / self.WINDOW) * self.WINDOW\n",
    "                for ii in range(0, actual_len, self.WINDOW ) :\n",
    "                    try:\n",
    "                        z = x[ ii : ii + self.WINDOW ]\n",
    "                    except:\n",
    "                        z = x[ ii : ]\n",
    "                    assert len(set(z[\"label\"].values[:])), \"[Error] Re-sampling is not OK\"\n",
    "\n",
    "                    feature = [file_id__, subject_id__, series_id__, z[\"time\"].values[0]]\n",
    "                    for v in [ np.array( z[u] ) for u in title ] :\n",
    "                        u = np.mean( v )\n",
    "                        feature += [u]\n",
    "                    feature += [z[\"label\"].values[0]]\n",
    "                    y.loc[len(y)] = feature\n",
    "                y = y.astype({'file_id': 'int64', \n",
    "                                'subject_id': 'int64',\n",
    "                                'series_id': 'int64',\n",
    "                                'label' : 'int64'})             \n",
    "                self.test.append(y) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nm-igkCItLY",
    "outputId": "33cf0611-d8df-4a73-db1c-a5ca0cd81b8f"
   },
   "outputs": [],
   "source": [
    "featureObj = Features()\n",
    "featureObj.run(filterObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLDWpxLg11_S",
    "outputId": "38f66b77-4638-4537-feec-bc00caf18f32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"[*] Number of sample original data: {len(filterObj.training[8])}\")\n",
    "print(f\"[*] Number of sample down sampling data: {len(featureObj.training[8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV5SwR0CItLa"
   },
   "source": [
    "### [*] Run the bellow code to get save/load features (downsampling data) to/from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tsDuXVP11_T"
   },
   "outputs": [],
   "source": [
    "# save the data as csv file\n",
    "path_features_training = \"../data/output/features-training.csv\"\n",
    "path_features_test = \"../data/output/features-test.csv\"\n",
    "\n",
    "for i, df in enumerate(featureObj.training):\n",
    "    if i:\n",
    "        df.to_csv(path_features_training, mode='a', header=False, index=False)\n",
    "    else: \n",
    "        df.to_csv(path_features_training, mode='w', header=True, index=False)\n",
    "\n",
    "for i, df in enumerate(featureObj.test):\n",
    "    if i:\n",
    "        df.to_csv(path_features_test, mode='a', header=False, index=False)\n",
    "    else: \n",
    "        df.to_csv(path_features_test, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDBUouGVItLb"
   },
   "outputs": [],
   "source": [
    "# run the code to LOAD THE DATA from csv file\n",
    "path_features_training = \"../data/output/features-training.csv\"\n",
    "path_features_test = \"../data/output/features-test.csv\"\n",
    "\n",
    "featureObjBackUp = Features()\n",
    "feature_training = pandas.read_csv(path_features_training)  \n",
    "for file_id in set(feature_training[\"file_id\"].values):\n",
    "    feature_training_part = feature_training[feature_training.file_id == file_id].sort_values(by=['time'])\n",
    "    featureObjBackUp.training.append(feature_training_part)\n",
    "    \n",
    "feature_test = pandas.read_csv(path_features_test)  \n",
    "for file_id in set(feature_training[\"file_id\"].values):\n",
    "    feature_test_part = feature_test[feature_test.file_id == file_id].sort_values(by=['time'])\n",
    "    featureObjBackUp.test.append(feature_test_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjP33ZDj11_b"
   },
   "source": [
    "# III. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzkjZ0nt11_b"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P69mkG6m11_b"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZ0qXt8J11_c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNWU_Oxv11_c"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "#torch.cuda.set_device(0)  # if you have more than one CUDA device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPcwCjUn11_c"
   },
   "source": [
    "### III.1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4GuF_RK11_c"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REzMwbSZhhv9"
   },
   "outputs": [],
   "source": [
    "# run the code to LOAD THE DATA from csv file\n",
    "path_features_training = \"../data/output/features-training.csv\"\n",
    "path_features_test = \"../data/output/features-test.csv\"\n",
    "\n",
    "feature_training = pandas.read_csv(path_features_training)  \n",
    "TRAIN = []\n",
    "for file_id in set(feature_training[\"file_id\"].values):\n",
    "    feature_training_part = feature_training[feature_training.file_id == file_id].sort_values(by=['time'])\n",
    "    TRAIN.append(feature_training_part)\n",
    "    \n",
    "feature_test = pandas.read_csv(path_features_test)  \n",
    "TEST = []\n",
    "for file_id in set(feature_training[\"file_id\"].values):\n",
    "    feature_test_part = feature_test[feature_test.file_id == file_id].sort_values(by=['time'])\n",
    "    TEST.append(feature_test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iW80PlPMh7_x",
    "outputId": "3ae455fc-f37c-40c5-ef57-93fa7fec0175"
   },
   "outputs": [],
   "source": [
    "TRAIN[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "Mn23jdiUiAOe",
    "outputId": "734d143d-7d1b-4a93-9bc8-bbf5acebf24a"
   },
   "outputs": [],
   "source": [
    "TEST[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWNQJtiiOtyr"
   },
   "source": [
    "### Examine the class label imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAmocslg9aJI"
   },
   "outputs": [],
   "source": [
    "SIZE_WINDOW = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ZfuHNUd-11_d",
    "outputId": "47ccbbbf-11c3-447c-9d62-2da7bb15163a"
   },
   "outputs": [],
   "source": [
    "x_titles = [ii for ii in TRAIN[0].keys() if ii not in ['index', 'file_id', 'subject_id', 'series_id', 'time', 'measurements', 'label']]\n",
    "x_train = [train__[x_titles] for train__ in TRAIN]\n",
    "x_train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "IDZmZ5RjO0_5",
    "outputId": "9469e77f-cfbf-40f7-dd45-e2cf8776d300"
   },
   "outputs": [],
   "source": [
    "y_titles = [ii for ii in TRAIN[0].keys() if ii in ['label']]\n",
    "y_train = [train__[y_titles].iloc[SIZE_WINDOW:] for train__ in TRAIN]\n",
    "y_train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6o3vE4AO2sy"
   },
   "outputs": [],
   "source": [
    "def create_dataset(x_train, y_train, valid_size = 0.2):\n",
    "    \n",
    "    y_train_list = []\n",
    "    x_dataset_list = []\n",
    "    for id_file in range(len(x_train)):\n",
    "        x_train__ = x_train[id_file]\n",
    "        y_train__ = y_train[id_file]\n",
    "\n",
    "        assert len(x_train__) == len(y_train__) + SIZE_WINDOW, \"The x_train and y_train is not match\"\n",
    "        # print(\"aaaaa\")        \n",
    "        for id_measurement in range(len(x_train__)):\n",
    "            if id_measurement < SIZE_WINDOW:\n",
    "                continue        \n",
    "            x_train_sample__ = x_train__.iloc[id_measurement - SIZE_WINDOW: id_measurement].to_numpy().reshape(16, 6)\n",
    "            x_dataset_list.append(x_train_sample__)\n",
    "        # print(\"bbbbb\")        \n",
    "        y_train_list.extend(y_train__[\"label\"].values)\n",
    "    # x_dataset = np.array(x_dataset_list)\n",
    "    x_dataset = x_dataset_list\n",
    "    # print(\"ccccc\")        \n",
    "    enc = LabelEncoder()\n",
    "    y_dataset = enc.fit_transform(y_train_list)\n",
    "\n",
    "    \n",
    "    assert len(x_dataset) == len(y_dataset), \"The x_dataset and y_dataset is not match\"\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(x_dataset, y_dataset, test_size=valid_size)\n",
    "    X_train, X_valid = [torch.tensor(arr, dtype=torch.float32) for arr in (X_train, X_valid)]\n",
    "    y_train, y_valid = [torch.tensor(arr, dtype=torch.long) for arr in (y_train, y_valid)]\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    valid_ds = TensorDataset(X_valid, y_valid)\n",
    "    return train_ds, valid_ds, enc\n",
    "\n",
    "def create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n",
    "    return train_dl, valid_dl\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    return (output.argmax(dim=1) == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffvvyU2VO4OU"
   },
   "outputs": [],
   "source": [
    "train_ds, valid_ds, enc = create_dataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hizjQngMpZOr",
    "outputId": "f99b56ec-6039-41cf-a0d4-1a99024b0945"
   },
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ed9-57thO5mo",
    "outputId": "2db885dc-bc8f-4442-fd9a-2c28ab693350"
   },
   "outputs": [],
   "source": [
    "bs = 128\n",
    "print(f'Creating data loaders with batch size: {bs}')\n",
    "trn_dl, val_dl = create_loaders(train_ds, valid_ds, bs, jobs=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4VgJvbF11_q"
   },
   "source": [
    "### III.2 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GwKd7gZ11_r"
   },
   "source": [
    "#### Cyclic Learning Rate\n",
    "\n",
    "The recent papers by L. Smith show us that the cyclic learning rate schedulers have very positive influence on model's convergence speed. In the following cells, we implement a simple cosine scheduler for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzY2wzBO11_r"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Op9p1PA811_r"
   },
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktgk2mEq11_r",
    "outputId": "4146a2d5-b0f5-46f3-fa8d-6724165c9d2d"
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "sched = cosine(n)\n",
    "lrs = [sched(t, 1) for t in range(n * 4)]\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghf11Ud111_r"
   },
   "source": [
    "#### The LSTM Model\n",
    "\n",
    "Our classifier contains of several LSTM cells (hidden under the hood of `nn.LSTM`),  and one `nn.Linear` layer. Note that we use `batch_first=True` to make sure that the first dimension of our tensors is interpreted as a batch size, and the next one - as a time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZnoxV4Y4uVr"
   },
   "outputs": [],
   "source": [
    "## HOLD ON TO RUNNING THIS CELL\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Checking if CUDA is available\n",
    "flag_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kHM1_g011_r"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Very simple implementation of LSTM-based time-series classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.actv = nn.Tanh()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        lstm_out, _,  = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(self.actv(self.dropout(lstm_out[:, -1, :])))\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        if not flag_cuda:\n",
    "            return [t for t in (torch.zeros(self.layers, x.size(0), self.hidden_size), torch.zeros(self.layers, x.size(0), self.hidden_size))]\n",
    "        else:\n",
    "            return [t.cuda() for t in (torch.zeros(self.layers, x.size(0), self.hidden_size), torch.zeros(self.layers, x.size(0), self.hidden_size))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIcdy9ye11_s"
   },
   "source": [
    "#### Training Loop\n",
    "\n",
    "Finally, we are ready to bring everything together and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucswacQW11_t",
    "outputId": "52e8f247-236b-4610-813b-58dac652d081",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_dim = 6  \n",
    "hidden_dim = 256\n",
    "layer_dim = 3\n",
    "output_dim = 4\n",
    "seq_dim = 16\n",
    "\n",
    "lr = 0.0001\n",
    "n_epochs = 150\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "best_acc = 0\n",
    "patience, trials = 300, 0\n",
    "\n",
    "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "sched = CyclicLR(opt, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    for i, (x_batch, y_batch) in enumerate(trn_dl):\n",
    "        model.train()\n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        out = model(x_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "        \n",
    "        train_loss += loss.item() * x_batch.size(0)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x_val, y_val in val_dl:\n",
    "        x_val, y_val = [t.cuda() for t in (x_val, y_val)]\n",
    "        out = model(x_val)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        total += y_val.size(0)\n",
    "        correct += (preds == y_val).sum().item()\n",
    "    \n",
    "        loss = criterion(out, y_val)\n",
    "        valid_loss += loss.item()*x_val.size(0)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    # Printing training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "      epoch, train_loss, valid_loss))\n",
    "\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), f'../data/output/model/best_{best_acc:2.2%}.pth')\n",
    "        print(f'Best model saved with accuracy: {best_acc:2.2%}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break\n",
    "print(\"-----------------------\\nTraining is done!!\")\n",
    "\n",
    "\n",
    "path_training_loss = \"../data/output/training_loss_modify.csv\"\n",
    "np.savetxt(path_training_loss, \n",
    "          [str(train_loss) for train_loss in train_loss_list], \n",
    "           delimiter =\"\\n\",  \n",
    "           fmt ='% s') \n",
    "path_valid_loss = \"../data/output/valid_loss_modify.csv\"\n",
    "np.savetxt(path_valid_loss, \n",
    "          [str(valid_loss) for valid_loss in valid_loss_list], \n",
    "           delimiter =\"\\n\",  \n",
    "           fmt ='% s') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "path_training_loss = \"../data/output/training_loss_modify.csv\"\n",
    "path_valid_loss = \"../data/output/valid_loss_modify.csv\"\n",
    "\n",
    "train_loss_load = genfromtxt(path_training_loss, delimiter =\"\\n\")\n",
    "valid_loss_load = genfromtxt(path_valid_loss, delimiter =\"\\n\")\n",
    "\n",
    "# Plotting the learning curves\n",
    "epochs_list = range(1, n_epochs + 1)\n",
    "plt.plot(epochs_list, train_loss_load, epochs_list, valid_loss_load)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.title(\"Performance of LSTM model. Training loss vs. Validation loss\")\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('learning_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thofODRUucRE"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "8PVIfCnDua70",
    "outputId": "5f7110e6-ca4a-41fa-fe35-df7c89e8490f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load(\"../data/output/model/best_99.39%.pth\"))\n",
    "model.eval()\n",
    "\n",
    "nb_classes = 4\n",
    "confussion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "with torch.no_grad():\n",
    "    for x_val, y_val in val_dl:\n",
    "        x_val, y_val = [t.cuda() for t in (x_val, y_val)]\n",
    "        out = model(x_val)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        # get accuracy    \n",
    "        total += y_val.size(0)\n",
    "        correct += (preds == y_val).sum().item()\n",
    "    \n",
    "        # get confussion matrix\n",
    "        for t, p in zip(y_val.view(-1), preds.view(-1)):\n",
    "            confussion_matrix[t.long(), p.long()] += 1  \n",
    "\n",
    "print(f\"Accuracy: {correct / total}%\")\n",
    "print(f\"Confussion Matrix\")\n",
    "plt.figure(figsize = (10,7))\n",
    "sns_plot = sns.heatmap(confussion_matrix, annot=True, cmap =sns.cm.rocket_r,linecolor='white', linewidths=1)\n",
    "plt.savefig(\"confussion_matrix.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEbKYNo64uVs"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF4y8A214uVt"
   },
   "outputs": [],
   "source": [
    "seed =  1\n",
    "np.random.seed(seed)\n",
    "#torch.cuda.set_device(0)  # if you have more than one CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TCZz9zE4uVt"
   },
   "outputs": [],
   "source": [
    "# run the code to LOAD THE DATA from csv file\n",
    "path_features_test = \"../data/output/features-test.csv\"\n",
    "\n",
    "feature_test = pandas.read_csv(path_features_test)  \n",
    "TEST = []\n",
    "for file_id in set(feature_test[\"file_id\"].values):\n",
    "    feature_test_part = feature_test[feature_test.file_id == file_id].sort_values(by=['time'])\n",
    "    TEST.append(feature_test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJuzv42g4uVt"
   },
   "outputs": [],
   "source": [
    "SIZE_WINDOW = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjvlWISO4uVt"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Very simple implementation of LSTM-based time-series classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.actv = nn.Tanh()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        lstm_out, _,  = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(self.actv(self.dropout(lstm_out[:, -1, :])))\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        if not flag_cuda:\n",
    "            return [t for t in (torch.zeros(self.layers, x.size(0), self.hidden_size), torch.zeros(self.layers, x.size(0), self.hidden_size))]\n",
    "        else:\n",
    "            return [t.cuda() for t in (torch.zeros(self.layers, x.size(0), self.hidden_size), torch.zeros(self.layers, x.size(0), self.hidden_size))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNSEsAhW4uVt",
    "outputId": "cb7abbde-b382-42c0-a33d-6346a12c49a6"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dim = 6  \n",
    "hidden_dim = 256\n",
    "layer_dim = 3\n",
    "output_dim = 4\n",
    "seq_dim = 16\n",
    "\n",
    "\n",
    "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load(\"../data/output/model/best_99.39%.pth\")) ### Change this appropciately to the path to your best model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# setting parameters for post-process\n",
    "FILTER_OUTPUT = True\n",
    "SLIDING_WINDOW = 21\n",
    "ACCEPT_THRESHOLD_ZEROS = 55 # 55%\n",
    "ACCEPT_THRESHOLD_OTHERS = 50 # 50%\n",
    "\n",
    "\n",
    "# Apply post-process to get the prediction bettet. Change FILTER_OUTPUT = True\n",
    "def post_process(label_file__, NUM_INTERATION = 1): \n",
    "    label_file = label_file__.copy()\n",
    "    for i in range(NUM_INTERATION):\n",
    "        for i in range(len(label_file)):\n",
    "            if i < SLIDING_WINDOW//2:\n",
    "                continue\n",
    "            label_list = label_file[i-SLIDING_WINDOW//2 : i + SLIDING_WINDOW//2]\n",
    "            counter = Counter(label_list)\n",
    "            keys = list(counter.keys())\n",
    "            values = list(counter.values())\n",
    "            \n",
    "            if len(keys) == 0:\n",
    "                continue\n",
    "\n",
    "            max_key = keys[values.index(sorted(values)[-1])]\n",
    "            # max_second_key = keys[values.index(sorted(values)[-2])]\n",
    "            # not fair between label 0 and label 1, 2, 3\n",
    "            if label_file[i] != max_key:\n",
    "                if max_key == 0 and int(counter[max_key]/SLIDING_WINDOW*100) > ACCEPT_THRESHOLD_ZEROS or \\\n",
    "                    max_key != 0 and int(counter[max_key]/SLIDING_WINDOW*100) > ACCEPT_THRESHOLD_OTHERS:\n",
    "                    label_file[i] = max_key\n",
    "                    \n",
    "    return label_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LV1gqEY4uVu",
    "outputId": "ddeba8ea-ff6a-4f9f-8cb8-c6098af74302"
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "import csv\n",
    "\n",
    "x_titles = [ii for ii in TEST[0].keys() if ii not in ['index', 'file_id', 'subject_id', 'series_id', 'time', 'measurements', 'label']]\n",
    "\n",
    "label_files = []\n",
    "for test__ in TEST:\n",
    "    \n",
    "    subject = test__['subject_id'].values[0]\n",
    "    path_file_csv = \"../data/output/subject_{:03d}_01__y.csv\".format(subject) # change this to the output folder as you will\n",
    "    print(path_file_csv)\n",
    "    \n",
    "    x_test__ = test__[x_titles]\n",
    "    #print(x_test__.head())\n",
    "\n",
    "    label_list = []\n",
    "    for id_measurement in range(len(x_test__)):\n",
    "        if id_measurement < SIZE_WINDOW:\n",
    "            label = 0\n",
    "        else:\n",
    "            x_test_sample__ = x_test__.iloc[id_measurement - SIZE_WINDOW: id_measurement].to_numpy().reshape(16, 6)\n",
    "            x_test_sample_tensor = torch.tensor(x_test_sample__, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "            # x_test_sample_tensor = torch.tensor(x_test_sample__, dtype=torch.float32).unsqueeze(0)\n",
    "            output = model(x_test_sample_tensor)\n",
    "            label = F.log_softmax(output, dim=1).argmax(dim=1).item()\n",
    "        label_list.append(label)\n",
    "    label_files.append(label_list)\n",
    "    \n",
    "    \n",
    "    with open(path_file_csv, 'w') as myfile:\n",
    "        wr = csv.writer(myfile, delimiter=',')\n",
    "        wr.writerow(label_list)\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "x_titles = [ii for ii in TEST[0].keys() if ii not in ['index', 'file_id', 'subject_id', 'series_id', 'time', 'measurements', 'label']]\n",
    "\n",
    "label_files = []\n",
    "for test__ in TEST:\n",
    "    \n",
    "    subject = test__['subject_id'].values[0]\n",
    "    \n",
    "    x_test__ = test__[x_titles]\n",
    "\n",
    "    label_list = []\n",
    "    for id_measurement in range(len(x_test__)):\n",
    "\n",
    "        if id_measurement < SIZE_WINDOW or  id_measurement > len(x_test__) - 2 - SIZE_WINDOW:\n",
    "            label = 0\n",
    "        else:\n",
    "            x_test_sample__ = x_test__.iloc[id_measurement - SIZE_WINDOW: id_measurement].to_numpy().reshape(16, 6)\n",
    "            if flag_cuda:\n",
    "                x_test_sample_tensor = torch.tensor(x_test_sample__, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "            else:\n",
    "                x_test_sample_tensor = torch.tensor(x_test_sample__, dtype=torch.float32).unsqueeze(0)\n",
    "            output = model(x_test_sample_tensor)\n",
    "            label = F.log_softmax(output, dim=1).argmax(dim=1).item()\n",
    "        label_list.append(label)\n",
    "\n",
    "    if FILTER_OUTPUT:\n",
    "        label_list__ = post_process(label_list)\n",
    "        # path_file_csv = os.path.join(PATH_RESULT, \"subject_{:03d}_01__y_filter_{}_{}.csv\".format(subject, ACCEPT_THRESHOLD_OTHERS, ACCEPT_THRESHOLD_ZEROS))\n",
    "        path_file_csv = \"../data/output/subject_{:03d}_01__y.csv\".format(subject)\n",
    "    else:\n",
    "        label_list__ = label_list\n",
    "        path_file_csv = \"../data/output/subject_{:03d}_01__y.csv\".format(subject)\n",
    "    print(path_file_csv)\n",
    "    with open(path_file_csv, 'w') as myfile:\n",
    "        wr = csv.writer(myfile, delimiter=',')\n",
    "        for label_list___ in label_list__:\n",
    "            wr.writerow([label_list___])\n",
    "    label_files.append(label_list__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject9 = label_files[0]\n",
    "print(len(subject9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 9498, 1)\n",
    "plt.figure()\n",
    "plt.plot(x, subject9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject10 = label_files[1]\n",
    "print(len(subject10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 12270, 1)\n",
    "plt.figure()\n",
    "plt.plot(x, subject10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject11 = label_files[2]\n",
    "print(len(subject11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 12940, 1)\n",
    "plt.figure()\n",
    "plt.plot(x, subject11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject12 = label_files[3]\n",
    "print(len(subject12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 11330, 1)\n",
    "plt.figure()\n",
    "plt.plot(x, subject12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ece-542-competition-lstm-Khoa-missing-labels.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "f7d6710f4b65747ad18016bd6506d7ecc09c14d10e28cf28270f528c2f5fdf51"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
